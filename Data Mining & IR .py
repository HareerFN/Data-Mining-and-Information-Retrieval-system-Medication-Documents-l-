# -*- coding: utf-8 -*-
"""Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1508s_8iq1q5MhmoYnEJCfozvK6JXdrcT

# **1-IMPORTING & LOAD**
"""

from google.colab import drive
import glob
from pathlib import Path

text_files = glob.glob(f"{'/content/drive/MyDrive/ALL notebooks'}/*.txt")
text_files.sort()
text_titles = [Path(text).stem for text in text_files]
import numpy as np
from numpy.linalg import norm

"""# **2-INDEXING**"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from gensim.utils import tokenize

nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def normalize(text):       # contain the: TOKENIZE -LOWRING- STOPWORDS REMOVEING -STEMMEING-LEMMATIZEING
    tokens = tokenize(text.lower())
    filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    stemmed_tokens = [ps.stem(word) for word in filtered_tokens]
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]
    return lemmatized_tokens

"""# **3- print the indexing docs**"""

import string
original_docs = []
normalized_docs = []
new_list =[]

for txt_file in text_files:
    with open(txt_file) as f:
        txt_file_as_string = f.read()
        original_doc = txt_file_as_string  # Save original document separately
        normalized_doc = normalize(txt_file_as_string)
        original_docs.append(original_doc)  # Append original document to original_docs
        normalized_docs.append(normalized_doc)
        new_list.append(normalized_docs)


print(*normalized_docs, sep='\n')

"""# **4- print the extraction features**"""

new_list

"""# **5-CALCULATE THE : IDF/TF/TF-IDF**"""

import math
def counting_docs(w, docs):
    count = sum(1 for doc in docs if w in doc)
    return count

def calc_tf_idf(document, word_dict):
    tf_idf = []
    for doc in document:
        row = []
        for word in word_dict:
            tf = doc.count(word) / len(doc)
            row.append(round(tf * calc_idf(word, document), 4))
        tf_idf.append(row)
    return tf_idf

def calc_idf(word, documents):
    n = len(documents)
    count = sum(1 for doc in documents if word in doc)
    return math.log10(n / (count + 1))

"""# **6-GENERATE TF-IDF** **DATAFRAME**"""

from sklearn.feature_extraction.text import TfidfVectorizer
new_list = [item for sublist in new_list for item in sublist]
import pandas as pd
flat_list = []
for sublist in normalized_docs:
    for item in sublist:
        flat_list.append(item)
wordDict = list(dict.fromkeys(flat_list))

TF_IDF = calc_tf_idf(normalized_docs, wordDict)
df = pd.DataFrame(TF_IDF, index = text_titles , columns = wordDict)

df

"""# **7-QUERY**"""

def Calc_Q_IDF (query, doc, wordDict):
  Q_IDF = []
  for w in wordDict:
    if (query.count(w) == 0):
      Q_IDF.append (float (0))
    else:
      Q_IDF.append (round(query.count(w)*calc_idf(w, doc),3))
  return (Q_IDF)
print("Enter the query")
query = input()
normalized_query = normalize (query)

QTF = Calc_Q_IDF (normalized_query, normalized_docs, wordDict)
query_df = pd.DataFrame(QTF)
query_df

"""# **8- similarities:**

# a- JACCARD (RANDOM DOCS)
"""

from sklearn.metrics import jaccard_score
import random
num_documents = len(normalized_docs)
similarities = []
random.shuffle(text_titles)

for i in range(num_documents):
    for j in range(i + 1, num_documents):
        if normalized_docs[i] and normalized_docs[j]:  # Check if both documents are non-empty
            if len(normalized_docs[i]) == len(normalized_docs[j]):  # Check if they have the same length
                similarity = jaccard_score(list(normalized_docs[i]), list(normalized_docs[j]), average="micro")
                similarities.append((text_titles[i], text_titles[j], similarity))

# Print the similarities
for title1	, title2, similarity in similarities:
    print(f"Similarity between '{title1	}' and '{title2}': {similarity}")

"""# b- COSINE SIMLARITY (DATAFRAME / PLOTTING)  /WITHOUT LIBRARY"""

import numpy as np
from numpy.linalg import norm
import numpy as np
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

def cosine_similarity(query_vector, document_vectors):
    similarities = []
    query_norm = np.linalg.norm(query_vector)


    for doc_vector in document_vectors:
        doc_norm = np.linalg.norm(document_vectors)
        if doc_norm != 0:
            similarity = np.dot(query_vector, doc_vector) / (query_norm * doc_norm)
        else:
            similarity = 0
        similarities.append(similarity)

    return similarities
cos=cosine_similarity(QTF,TF_IDF)
cos_sim_df = pd.DataFrame(cos)
print(cos_sim_df)



similarity_matrix = cosine_similarity(QTF, TF_IDF)

# Convert the list to a numpy array
similarity_matrix = np.array(similarity_matrix)

# Ensure similarity_matrix is a 2D array
if similarity_matrix.ndim == 1:
    similarity_matrix = similarity_matrix.reshape(1, -1)

# Plot the similarity matrix
plt.figure(figsize=(8, 6))
plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')
plt.title('Cosine Similarity Matrix')
plt.colorbar()
plt.show()

"""# c- DICE SIMILARITY (DATAFRAME) /WITHOUT LIBRARY"""

def dice_similarity(query_vector, document_vectors):
    similarities = []
    query_norm2 = np.linalg.norm(query_vector)

    for doc_vector in document_vectors:
        doc_norm = np.linalg.norm(doc_vector)
        if doc_norm != 0:
            query_norm2 = np.power(query_norm2, 2)
            doc_norm = np.power(doc_norm, 2)
            similarity =  2 * ( np.dot(query_vector, doc_vector) ) / (query_norm2 + doc_norm)
        else:
            similarity = 0
        similarities.append(similarity)

    return similarities

dice=dice_similarity(QTF,TF_IDF)
dice_sim_df = pd.DataFrame(dice)
dice_sim_df

"""# **9-RANKING**

# a- RANKING between the similarities
"""

# Ensure cos and dice arrays are not empty------ this error occur
max_cos = np.max(cos) if len(cos) > 0 else 0
max_dice = np.max(dice) if len(dice) > 0 else 0

# Find index of max similarity in each array
index_cos = np.argmax(cos) if len(cos) > 0 else -1
index_dice = np.argmax(dice) if len(dice) > 0 else -1
# Retrieve corresponding titles
title_cos = text_titles[index_cos] if index_cos != -1 else "No title"
title_dice = text_titles[index_dice] if index_dice != -1 else "No title"
# Find the overall maximum similarity
MAXIMUM = max(max_cos, max_dice)
print(f"Maximum cosine similarity: {max_cos * 100:.4f}% for title: {title_cos}")
print(f"Maximum dice similarity: {max_dice * 100:.4f}% for title: {title_dice}")

"""#b- RANKING the top 5 invidual similarity"""

import numpy as np

# Check if the arrays are not empty
if len(cos) > 0 and len(dice) > 0:
    # Compute max similarity for cosine and dice
    max_cos = np.max(cos)
    max_dice = np.max(dice)
    MAXIMUM = max(max_cos, max_dice)

    # Find indices of top 5 max similarities in each array
    top_indices_cos = np.argsort(cos)[-5:][::-1]  # Top 5 indices for cosine similarity
    top_indices_dice = np.argsort(dice)[-5:][::-1]  # Top 5 indices for dice similarity

    # Retrieve corresponding titles for cosine similarity
    top_titles_cos = [text_titles[i] for i in top_indices_cos]

    # Retrieve corresponding titles for dice similarity
    top_titles_dice = [text_titles[i] for i in top_indices_dice]

    # Print top 5 similarities and corresponding titles for cosine similarity
    print("Top 5 Cosine Similarities and Titles:")
    for i, (index, title) in enumerate(zip(top_indices_cos, top_titles_cos), 1):
        print(f"{i}. Similarity: {cos[index]}, Title: {title}")

    # Print top 5 similarities and corresponding titles for dice similarity
    print("\nTop 5 Dice Similarities and Titles:")
    for i, (index, title) in enumerate(zip(top_indices_dice, top_titles_dice), 1):
        print(f"{i}. Similarity: {dice[index]}, Title: {title}")
else:
    print("Error: One or both similarity arrays are empty.")

"""# c- RANKING the top 5 of ALL Similarities"""

combined_list = np.add(cos, dice)  # Combined similarity scores
top_indices = np.argsort(combined_list)[-5:][::-1]  # Top 5 indices for all similarities

print("Top 5 Similarities and Titles:")
for i, index in enumerate(top_indices, 1):
    title = text_titles[index]
    similarity = combined_list[index]
    print(f"{i}. Similarity: {similarity}, Title: {title}")

"""**bold text**# **10- COMPARE BETWEEN SIMILARITIES (PLOTTING)**"""

import matplotlib.pyplot as plt
y1=cos
y2=dice
plt.plot(y1, 'g*', label='cosine sim ')
plt.plot( y2, 'ro',label=' dice sim ')
plt.show()
plt.plot(y1, 'g', label='cosine sim ')
plt.plot(y2, 'r', label=' dice sim ')
plt.legend()
plt.show()

"""# **11-RECALL&PRECISION**"""

relvent=0
retrived=0
for doc_vector in dice:
  retrived+=1
  if doc_vector!=0:
   relvent+=1


print(retrived,relvent)
def calculate_precision_recall(retrieved_docs, relevant_docs):

    retrieved_set = set(range(1, retrieved_docs + 1))  # Assuming document IDs start from 1
    relevant_set = set(range(1, relevant_docs + 1))    # Assuming document IDs start from 1

    # Calculate true positives
    true_positives = len(retrieved_set.intersection(relevant_set))
    # Calculate false negatives
    false_negatives =5
    # len(relevant_set.difference(retrieved_set))
    # Calculate false positives
    false_positives = len(retrieved_set.difference(relevant_set))

    # Calculate precision
    precision = true_positives /(true_positives+false_positives)
   # precision = true_positives / retrieved_docs if retrieved_docs != 0 else 0

    # Calculate recall
    recall = true_positives/(true_positives+false_negatives)
    #recall = true_positives / relevant_docs if relevant_docs != 0 else 0
    return precision, recall

retrieved = retrived
relvent = relvent

precision, recall = calculate_precision_recall(retrieved, relvent)

print("Precision:", precision)
print("Recall:", recall)

"""# **12- R&P(PLOTTING)**"""

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='o')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)
plt.show()

from nltk.tokenize import  TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,WordNetLemmatizer
nltk.download('punkt')

preprocessed_documents = []
idf_tf_data = []
global new_document
for text_file in text_files:
    with open(text_file, 'r') as file:
        document = file.read()
        tokens_treebank = tokenizer.tokenize(document)
        print("TOKENIZE")
        print(tokens_treebank)
        print("--------------------------------------------------------------")
        lower=document.lower()
        print("LOEWRING")
        print(lower)
        print("--------------------------------------------------------------")
        print("STOP WORDS")
        new_document = ' '.join([word for word in document.split() if word not in (stopwords.words('english'))])
        print(new_document)
        print("--------------------------------------------------------------")
        print("STEMMING")
        PS=PorterStemmer()
        print(PS.stem(new_document))
        print("--------------------------------------------------------------")
        print("LEMMATIZATION")
        LT=WordNetLemmatizer()
        print(LT.lemmatize(new_document))

        print("Old length: ", len(document))
        print("New length: ", len(new_document))

        preprocessed_documents.append(new_document)

!pip install normalizer


from normalizer import normalize


query = 'Panadol'


normalized_query = normalize(query)


QTF = Calc_Q_IDF(normalized_query, normalized_docs, wordDict)